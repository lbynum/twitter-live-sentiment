{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis -- Classification and Live Plotting\n",
    "\n",
    "![](images/live-twitter-sentiment.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and Problem Statement\n",
    "\n",
    "Our project aims to perform high-level sentiment analysis on Twitter data. If successful, our work will allow us to gain insights in real-time about how Twitter users feel about any given topic. Tweets are an important and widely used form of communication, not only providing a source of entertainment but also useful news updates. We hope to gain an understanding of how users feel about a topic and how this changes over time. Tweets are an interesting source of data by nature, constrained by the 140 character restriction but still able to leverage the variability of human language. Traditional methods for sentiment analysis and topic modelling may not perform as well on Tweets because they do not capture as much information as a full-length document. This project is interesting as a Big Data project because of the large number of daily new tweets (and other types of social media posts, were this work extended). A model, once trained, could be used to classify live social media data and understand and measure trends as they occur. Anyone interested in understanding such trends would use these types of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description\n",
    "\n",
    "We will develop our models using static data and after we establishing a working pipeline, we will integrate with tweepy to stream, classify, and visualize live Tweets.\n",
    "\n",
    "Our data comes from the SemEval-2017 Task 4: Sentiment Analysis in Twitter. This task publishes data for training sentiment analysis models related to Twitter data and has a popular competetion with about 50 teams competing in 2017. This dataset is fairly general and varied in terms of topics and was created with the intent of furthering work in the area of sentiment analysis. The dataset contains 16,041 tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Embedding\n",
    "\n",
    "We begin by making our sentiment analysis classification a binary task. That is, we classify tweets as either positive or negative. Therefore, we remove tweets from our filtered dataset that have the 'neutral' label. The size of the filtered dataset is 9201 tweets, and roughly 75% of the tweets are labelled positive while the remaining 25% are labelled negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "path_to_data = 'result.txt'\n",
    "all_tweets_df = pd.read_table(path_to_data, names = ['ID', 'sentiment', 'tweet'])\n",
    "\n",
    "# remove neutral tweets (for now)\n",
    "all_tweets_df = all_tweets_df[all_tweets_df['sentiment'] != 'neutral']\n",
    "\n",
    "# convert sentiment labels to binary (1 = positive, 0 = negative)\n",
    "all_tweets_df['sentiment'] = (all_tweets_df['sentiment'] == 'positive').astype(int)\n",
    "\n",
    "# split features and labels\n",
    "X = all_tweets_df['tweet']\n",
    "y = all_tweets_df['sentiment']\n",
    "\n",
    "# separate the training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "print('Training data dimensions n,d = {}'.format(X_train.shape))\n",
    "print('Test data dimensions n,d = {}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing: Punctuation, Stopwords, Lemmatization, Bag of Words, TF-IDF\n",
    "\n",
    "Our preliminary text processing for all tweets includes removing stopwords and accents and leveraging lemmatization.\n",
    "\n",
    "We decide to maintain the punctuation present in the original tweets. After initial exploration of our tweets dataset, we notice several examples of punctuation conveying meaning and we felt this could impact the overall sentiment (eg. `'!'`, `'...'`, `':)'`, `':('`, `'>>'`). Therefore, punctuation is included when we process each tweet.\n",
    "\n",
    "We then filter out common stopwords from our `CountVectorizer` vocabulary using `NLTK`'s built-in list of English stopwords. This is a rather simple way to remove the most commonly occuring stopwords. Additionally we remove accents from the text using `CountVectorizer`'s strip_accent parameter.\n",
    "\n",
    "We also utilize `NLTK`'s WordNetLemmatizer to perform lemmatization for each tweet. The goal of lemmatization is to transform each terms back into their base form. For example, given the terms 'walking', 'walked', 'walker', the lemmatizer would convert all of them back to 'walk'. The idea behind this is to represent terms that have different inflectional endings but capture the same meaning as a single base term. The lemmatizer we have used takes in an optional part of speech parameter. However, since we have chosen not to provide part of speech, the default is 'noun', meaning the lemmatizer will attempt to convert each term to the closest base in noun form.\n",
    "\n",
    "In terms of our document matrix, we first use the term frequencies provided by `CountVectorizer`. In this approach, also known as bag-of-words, each row corresponds to a document (tweet) and each column represents a unique term. Each element in the matrix corresponds to the count of the specified term for that particular document. While this bag-of-words model allows us to gain some insight regarding which terms are important in a given document, it does not have a means to look at term frequencies across different documents. For example a given term may occur frequently in a given document and therefore appear important, but then also appear frequently for several of the other documents as well.\n",
    "\n",
    "To address this, we can apply the TF-IDF transformation to the bag-of-words model. The term frequency measures the local importance of the word (standard bag-of-words), and the inverse document frequency which is equal to the $log(\\frac{a}{b})$ where $a$ = # of Documents and $b$ = # of Documents containing term $t$. Therefore, terms that occur highly within a given document but do not occur heavily in other documents have higher values. The product of the term frequency and inverse document frequency now make up the individual elements of the document matrix.\n",
    "\n",
    "Further text processing and embedding approaches are discussed below in the hyperparameter tuning section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Classification Models\n",
    "\n",
    "We could use several different types of models to classify our Twitter data as positive/negative (along with the different embedding strategies discussed above). Here we'll try the following classifiers: Multinomial Naive Bayes Classifiers, Logistic Regression Classifiers, Stochastic Gradient Descent Classifiers, and Support Vector Classifiers. We use `sklearn` to build several pipelines that allow us to chain different combinations of embedding strategies and classification models. After building them here, we'll use cross-validation to compare their performance and find the best combinations of embedding strategies and classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# embedding methods\n",
    "embedding_tuple = ('vect', CountVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                             stop_words='english',\n",
    "                             strip_accents='unicode'))\n",
    "tfidf_tuple = ('tfidf', TfidfTransformer())\n",
    "\n",
    "# classifiers\n",
    "mnb_tuple = ('clf', MultinomialNB())\n",
    "svm_tuple = ('clf', SVC())\n",
    "logreg_tuple = ('clf', LogisticRegression())\n",
    "sgd_tuple = ('clf', SGDClassifier())\n",
    "\n",
    "### Classifier Pipelines\n",
    "# Multinomial Naive Bayes\n",
    "tfidf__mnb_classifier = Pipeline([embedding_tuple, tfidf_tuple, mnb_tuple])\n",
    "bow__mnb_classifier = Pipeline([embedding_tuple, mnb_tuple])\n",
    "\n",
    "# SVC (linear and rbf)\n",
    "tfidf__svm_classifier = Pipeline([embedding_tuple, tfidf_tuple, svm_tuple])\n",
    "bow__svm_classifier = Pipeline([embedding_tuple, svm_tuple])\n",
    "\n",
    "# Logistic regression\n",
    "tfidf__logreg_classifier = Pipeline([embedding_tuple, tfidf_tuple, logreg_tuple])\n",
    "bow__logreg_classifier = Pipeline([embedding_tuple, logreg_tuple])\n",
    "\n",
    "# Stochastic gradient descent\n",
    "tfidf__sgd_classifier = Pipeline([embedding_tuple, tfidf_tuple, sgd_tuple])\n",
    "bow__sgd_classifier = Pipeline([embedding_tuple, sgd_tuple])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning via Cross-Validation\n",
    "\n",
    "With our model pipelines in place, we next use cross-validation to find the optimal parameters and embedding strategy for each. \n",
    "\n",
    "We leveraged `Sklearn`'s GridSearchCV to find the optimal paramters for each classifier given a dictionary of parameters and corresponding set of values using 3-fold cross validation. Each of the parameter tuning choices are discussed below:\n",
    "\n",
    "Text Processing (for all classifiers)\n",
    "+ Max-df: Removes terms that appear too frequently (eg. max_df = 0.5 means that we ignore terms that appear in more than 50% of the documents). We decided to tune with two values: 0.5 and 0.75.\n",
    "+ Max_features: Builds the model's vocabulary based on the top max_features. We decided to tune with three values: 2000, 5000, and None (does not restrict the number of features to be included in vocabulary).\n",
    "+ Ngram_range: Specifies n-gram combinations that will be included in the model's vocabulary. We decided to tune with three values: (1,1), (1,2), and (2,2). We felt that including longer n_grams such as trigrams would capture phrases unique to a specific tweet, but not generalizable to other tweets.\n",
    "\n",
    "Multinomial Naive Bayes and Stochastic Gradient Descent:\n",
    "+ No hyperparameter tuning for these models\n",
    "\n",
    "Logistic Regression\n",
    "\n",
    "+ C: Inversely proportional to regularization (smaller values indicate stronger regularization). Regularization essentially adds a penalty for overfitting to our training data. We decided to tune with the following values: 0.01, 0.1, 1, 10, 100.\n",
    "\n",
    "SVMs\n",
    "+ C: Similar to logistic regression, the C parameter indicates how much we will overfit to the training data. For large values of C, the optimization will choose a smaller-margin hyperplane (if that leads to better training classification performance). A very small value of C will cause the optimization to look for a larger-margin separating hyperplane, even if that leads to greater misclassification of training data. We decided to tune with the following values: 0.01, 0.1, 1, 10, 100.\n",
    "+ Gamma: The gamma parameter describes the influence of a single training example in determining the decision boundary (low values indicate examples further away from the decision boundary carry higher weight and high values indicate that closer carry higher weight). We decided to tune with the following values: 0.01, 0.1, 1, 10, 100.\n",
    "+ Kernel: We decided to tune with two kernels: linear (no kernel) and the rbf (radial basis function) kernel, which is the default option for `sklearn` SVM classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# First specify parameter dictionaries for the various classifiers\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75),\n",
    "    'vect__max_features': (None, 2000, 5000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2), (2,2)),  # unigrams or bigrams\n",
    "    'clf__C': (0.01, 0.1, 1, 10, 100),\n",
    "    'clf__gamma': (0.01, 0.1, 1, 10, 100),\n",
    "}\n",
    "\n",
    "parameters_lr = {\n",
    "    'vect__max_df': (0.5, 0.75),\n",
    "    'vect__max_features': (None, 2000, 5000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2), (2,2)),  # unigrams or bigrams\n",
    "    'clf__kernel': ('linear', 'rbf'),\n",
    "    'clf__C': (0.01, 0.1, 1, 10, 100),\n",
    "}\n",
    "\n",
    "# Use Grid Search CV to find optimal model parameters for each of the following classifiers:\n",
    "\n",
    "# Multinomial Bayes with TF-IDF\n",
    "print('Tuning Multinomial Bayes with TF-IDF')\n",
    "grid_search = GridSearchCV(tfidf__mnb_classifier, parameters, n_jobs=-1, verbose=1)\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters_svm.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# Multinomial Bayes with Bag of Words\n",
    "print('Tuning Multinomial Bayes with Bag of Words')\n",
    "grid_search = GridSearchCV(bow__mnb_classifier, parameters, n_jobs=-1, verbose=0)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# Multinomial# SVM with TF_IDF\n",
    "print('Tuning SVM with TF_IDF')\n",
    "grid_search = GridSearchCV(tfidf__svm_classifier, parameters, n_jobs=-1, verbose=0)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# SVM with Bag of Words\n",
    "print('Tuning SVM with Bag of Words')\n",
    "grid_search = GridSearchCV(bow__svm_classifier, parameters_svm, n_jobs=-1, verbose=0)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters_lr.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# Logistic Regression with TF_IDF\n",
    "print('Tuning Logistic Regression with TF_IDF')\n",
    "grid_search = GridSearchCV(tfidf__logreg_classifier, parameters_lr, n_jobs=-1, verbose=0)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters_svm.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# Logistic Regression with Bag of Words\n",
    "print('Tuning Logistic Regression with Bag of Words')\n",
    "grid_search = GridSearchCV(bow__logreg_classifier, parameters_lr, n_jobs=-1, verbose=0)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# SGD with TF_IDF\n",
    "print('Tuning SGD with TF_IDF')\n",
    "grid_search = GridSearchCV(tfidf__sgd_classifier, parameters_lr, n_jobs=-1, verbose=0)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters_lr.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# SGD with Bag of Words\n",
    "print('Tuning SGD with Bag of Words')\n",
    "grid_search = GridSearchCV(bow__sgd_classifier, parameters, n_jobs=-1, verbose=0)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Models with Optimal Parameters\n",
    "We will now use the parameters computed from above to train our classifiers. We will also predict on the test data to assess the performance of our classifiers.\n",
    "\n",
    "All the classifier performances are roughly between 80-85% for accuracy/precision and 90-95% for recall. Our classification performance is fairly high and consistent for each classifier. Furthermore, even though we have an imbalanced class situation with approx. 75% positive tweets, our classifiers go beyond choosing the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "tfidf_tuple = ('tfidf', TfidfTransformer())\n",
    "mnb_tuple = ('clf', MultinomialNB())\n",
    "sgd_tuple = ('clf', SGDClassifier())\n",
    "\n",
    "### Classifier Pipelines\n",
    "# Multinomial Naive Bayes\n",
    "tfidf_mnb_classifier = Pipeline([('vect', CountVectorizer(tokenizer=LemmaTokenizer(), stop_words='english', strip_accents='unicode', max_df=0.5, max_features=2000, ngram_range=(1, 2))), tfidf_tuple, mnb_tuple])\n",
    "\n",
    "bow_mnb_classifier = Pipeline([('vect', CountVectorizer(tokenizer=LemmaTokenizer(), stop_words='english', strip_accents='unicode', max_df=0.5, max_features=5000, ngram_range=(1, 2))), mnb_tuple])\n",
    "\n",
    "# Logistic regression\n",
    "tfidf_logreg_classifier = Pipeline([('vect', CountVectorizer(tokenizer=LemmaTokenizer(), stop_words='english', strip_accents='unicode', max_df=0.5, max_features=None, ngram_range=(1, 2))), tfidf_tuple, ('clf', LogisticRegression(C=100))])\n",
    "\n",
    "bow_logreg_classifier = Pipeline([('vect', CountVectorizer(tokenizer=LemmaTokenizer(), stop_words='english', strip_accents='unicode', max_df=0.5, max_features=5000, ngram_range=(1, 2))),('clf', LogisticRegression(C=1))])\n",
    "\n",
    "# Stochastic gradient descent\n",
    "tfidf_sgd_classifier = Pipeline([('vect', CountVectorizer(tokenizer=LemmaTokenizer(), stop_words='english', strip_accents='unicode', max_df=0.75, max_features=None, ngram_range=(1, 2))), tfidf_tuple, sgd_tuple])\n",
    "bow_sgd_classifier = Pipeline([('vect', CountVectorizer(tokenizer=LemmaTokenizer(), stop_words='english', strip_accents='unicode', max_df=0.5, max_features=None, ngram_range=(1, 2))), sgd_tuple])\n",
    "\n",
    "# Support Vector Machines\n",
    "tfidf_svm_classifier = Pipeline([('vect', CountVectorizer(tokenizer=LemmaTokenizer(), stop_words='english', strip_accents='unicode', max_df=0.5, max_features=None, ngram_range=(1, 2))), tfidf_tuple, ('clf', SVC(C=10, gamma=0.01, kernel='linear'))])\n",
    "bow_svm_classifier = Pipeline([('vect', CountVectorizer(tokenizer=LemmaTokenizer(), stop_words='english', strip_accents='unicode', max_df=0.5, max_features=None, ngram_range=(1, 1))), ('clf', SVC(C=10, gamma=0.01, kernel='rbf'))])\n",
    "\n",
    "# Fit each of the classifiers and pickle the fitted models\n",
    "print('Fitting Multinomial Bayes with Tf-idf')\n",
    "tfidf_mnb_classifier.fit(X_train, y_train)\n",
    "pred = tfidf_mnb_classifier.predict(X_test)\n",
    "print('here is the mnb tf-idf accuracy, precision, recall: ', [accuracy_score(y_test, pred), precision_score(y_test, pred), recall_score(y_test, pred)])\n",
    "\n",
    "print('Fitting Multinomial Bayes with bow')\n",
    "bow_mnb_classifier.fit(X_train, y_train)\n",
    "pred = bow_mnb_classifier.predict(X_test)\n",
    "print('here is the mnb bow accuracy, precision, recall: ', [accuracy_score(y_test, pred), precision_score(y_test, pred), recall_score(y_test, pred)])\n",
    "\n",
    "print('Fitting Logistic Regression with tf-idf')\n",
    "tfidf_logreg_classifier.fit(X_train, y_train)\n",
    "pred = tfidf_logreg_classifier.predict(X_test)\n",
    "print('here is the log reg tf-idf accuracy, precision, recall: ', [accuracy_score(y_test, pred), precision_score(y_test, pred), recall_score(y_test, pred)])\n",
    "\n",
    "print('Fitting Logistic Regression with bow')\n",
    "bow_logreg_classifier.fit(X_train, y_train)\n",
    "pred = bow_logreg_classifier.predict(X_test)\n",
    "print('here is the log reg bow accuracy, precision, recall: ', [accuracy_score(y_test, pred), precision_score(y_test, pred), recall_score(y_test, pred)])\n",
    "\n",
    "print('Fitting SGD with tf-idf')\n",
    "tfidf_sgd_classifier.fit(X_train, y_train)\n",
    "pred = tfidf_sgd_classifier.predict(X_test)\n",
    "print('here is the sgd tf-idf accuracy, precision, recall: ', [accuracy_score(y_test, pred), precision_score(y_test, pred), recall_score(y_test, pred)])\n",
    "\n",
    "print('Fitting SGD with bow')\n",
    "bow_sgd_classifier.fit(X_train, y_train)\n",
    "pred = bow_sgd_classifier.predict(X_test)\n",
    "print('here is the sgd bow accuracy, precision, recall: ', [accuracy_score(y_test, pred), precision_score(y_test, pred), recall_score(y_test, pred)])\n",
    "\n",
    "print('Fitting SVM with tf-idf')\n",
    "tfidf_svm_classifier.fit(X_train, y_train)\n",
    "pred = tfidf_svm_classifier.predict(X_test)\n",
    "print('here is the svm tf-idf accuracy, precision, recall: ', [accuracy_score(y_test, pred), precision_score(y_test, pred), recall_score(y_test, pred)])\n",
    "\n",
    "print('Fitting SVM with bow')\n",
    "bow_svm_classifier.fit(X_train, y_train)\n",
    "pred = bow_svm_classifier.predict(X_test)\n",
    "print('here is the svm bow accuracy, precision, recall: ', [accuracy_score(y_test, pred), precision_score(y_test, pred), recall_score(y_test, pred)])\n",
    "\n",
    "# dictionary that contains model name as key and fitted model as value\n",
    "models = {'tfidf__mnb_classifier': tfidf_mnb_classifier, 'bow__mnb_classifier': bow_mnb_classifier, 'tfidf__logreg_classifier': tfidf_logreg_classifier,'bow__logreg_classifier': bow_logreg_classifier, 'tfidf_sgd_classifier': tfidf_sgd_classifier, 'bow__sgd_classifier': bow_sgd_classifier,'tfidf__svm_classifier': tfidf_svm_classifier, 'bow__svm_classifier': bow_svm_classifier}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickling the Best Models\n",
    "\n",
    "Training our models takes a little while each time. We don't want to repeat this process every time we want to use them. Thus we'll pickle our models for use whenever we need them (such as in the next section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in items(models):\n",
    "    pickle.dump(model, open('pickled_classifiers/' + name + '.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a VoteClassifier\n",
    "\n",
    "With all of our models tuned appropriately via cross-validation, we can get a classification score along with some measure of confidence by aggregating the predictions of each model and doing a simple 'majority rule' vote. We first implement the `VoteClassifier` class to take a group of classification models and create a majority vote method (`classify()`) as well as a method to get a rough confidence for the given prediction -- simply the proportion of the group that agrees (`confidence()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import ClassifierI\n",
    "\n",
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, classifiers):\n",
    "        self._classifiers = classifiers\n",
    "        \n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.predict(features)[0]\n",
    "            votes.append(v)\n",
    "        mode = max(set(votes), key=votes.count)\n",
    "        return mode\n",
    "    \n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.predict(features)[0]\n",
    "            votes.append(v)\n",
    "        mode = max(set(votes), key=votes.count)\n",
    "        choice_votes = votes.count(mode)\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Twitter Data\n",
    "\n",
    "With a way to classify a tweet as positive/negative along with a rough confidence in our classification, we can apply the models we've trained and tuned to classifying data streaming live from Twitter. To achieve this, we use the `tweepy` library. Here we implement a `TwitterListener` class to stream and classify tweets, outputing the classification along with the confidence to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "\n",
    "CONSUMER_KEY = 'tOpKHcNzNpu88PSZxAzCI87Ne'\n",
    "CONSUMER_SECRET = 'EhSEN89oydJi058EkQP3iMjsVlYw6yLYZ2Uq2UAVSWS43wXju9'\n",
    "ACCESS_TOKEN = '37501551-hUS1bgjvyBq9H1pplnXkQb1rBIqNfNzPsBHFtx8dw'\n",
    "ACCESS_SECRET = '0MI19XnM6FXT8D8LWM70KbHYZDp5GefyZpYwD6hUUvtSD'\n",
    "\n",
    "# load pickled models and instantiate VoteClassifier\n",
    "models = []\n",
    "model_names = ['tfidf__mnb_classifier', 'bow__mnb_classifier', 'tfidf__logreg_classifier','bow__logreg_classifier', 'tfidf_sgd_classifier', 'bow__sgd_classifier', 'tfidf__svm_classifier', 'bow__svm_classifier']\n",
    "\n",
    "for name in model_names:\n",
    "    model = pickle.load(open('pickled_classifiers/' + name + '.pickle', 'rb'))\n",
    "    models.append(model)\n",
    "\n",
    "VOTECLASSIFIER = VoteClassifier(models)\n",
    "\n",
    "class TwitterListener(StreamListener):\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            json_data = json.loads(data)\n",
    "            tweet = json_data['text']\n",
    "            sentiment = 'positive' if VOTECLASSIFIER.classify([tweet]) else 'negative'\n",
    "            confidence = VOTECLASSIFIER.confidence([tweet])\n",
    "            print(sentiment, confidence)\n",
    "\n",
    "            with open('twitter-out.txt', 'a') as output:\n",
    "                output.write(str(sentiment) + ',' + str(confidence))\n",
    "                output.write('\\n')\n",
    "        except Exception as exception:\n",
    "            print(exception)\n",
    "            pass\n",
    "\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        pass\n",
    "\n",
    "# authenticate\n",
    "auth = OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Classification\n",
    "\n",
    "With our `TwitterListener` in place and authenticated to connect to Twitter, we're ready to predict whether each incoming tweet is positive or negative. The Twitter API allows us to filter tweets by language and topic. Here we'll consider only Tweets in English. The filtering by topic will allow us to help get a sense of how the public (or at least the subsection of the public with Twitter accounts) feels about a given topic. For this example, we'll see how the English-speaking Twitter population feels about Donald Trump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create streaming object\n",
    "topics_to_track = ['donald trump']\n",
    "twitterStream = Stream(auth, TwitterListener())\n",
    "twitterStream.filter(languages=['en'], track=topics_to_track)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Visualization\n",
    "\n",
    "The last step in answering our question is to plot our live results. We use `matplotlib`'s `pyplot` and `animation` libraries to read in the contents of the textfile containing our live results and display a histogram for positive and negative tweets as well as the mean confidence associated with each classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import style\n",
    "\n",
    "style.use(\"ggplot\")\n",
    "\n",
    "fig = plt.figure()\n",
    "subplot_1 = fig.add_subplot(2,2,1)\n",
    "subplot_2 = fig.add_subplot(2,2,2)\n",
    "subplot_3 = fig.add_subplot(2,1,2)\n",
    "\n",
    "def animate(i):\n",
    "    pullData = open(\"twitter-out.txt\",\"r\").read()\n",
    "    lines = pullData.split('\\n')\n",
    "\n",
    "    negative_count = 0\n",
    "    positive_count = 0\n",
    "    neg_confidence_list = []\n",
    "    pos_confidence_list = []\n",
    "\n",
    "    all_sentiments_list = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.split(',')\n",
    "        if len(line) > 1:\n",
    "\n",
    "            if line[0] == 'positive':\n",
    "                positive_count += 1\n",
    "                pos_confidence_list.append(float(line[1]))\n",
    "                all_sentiments_list.append(1)\n",
    "            elif line[0] == 'negative':\n",
    "                negative_count += 1\n",
    "                neg_confidence_list.append(float(line[1]))\n",
    "                all_sentiments_list.append(0)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "\n",
    "    # pos/neg plot\n",
    "    bar_heights = [negative_count, positive_count]\n",
    "    subplot_1.clear()\n",
    "    subplot_1.bar([1,2], bar_heights)\n",
    "    subplot_1.set_xticks([1, 2])\n",
    "    subplot_1.set_xticklabels(('NEG', 'POS'))\n",
    "    subplot_1.set_title('Pos/Neg Counts')\n",
    "\n",
    "    # confidence plot\n",
    "    neg_mean_confidence = 0\n",
    "    pos_mean_confidence = 0\n",
    "    if len(neg_confidence_list) > 0:\n",
    "        neg_mean_confidence = sum(neg_confidence_list) * 1.0 / len(neg_confidence_list)\n",
    "    if len(pos_confidence_list) > 0:\n",
    "        pos_mean_confidence = sum(pos_confidence_list) * 1.0 / len(pos_confidence_list)\n",
    "    bar_heights = [neg_mean_confidence, pos_mean_confidence]\n",
    "    subplot_2.clear()\n",
    "    subplot_2.bar([1,2], bar_heights)\n",
    "    subplot_2.set_xticks([1, 2])\n",
    "    subplot_2.set_xticklabels(('NEG', 'POS'))\n",
    "    subplot_2.set_title('Mean Confidence')\n",
    "\n",
    "    # moving average plot for last 200 values\n",
    "    lookback_number = min(len(all_sentiments_list), 200)\n",
    "    average_array = np.ones((lookback_number,)) / lookback_number\n",
    "    moving_averages = np.convolve(all_sentiments_list, average_array, mode='valid')\n",
    "    # look at most recent 50000\n",
    "    if len(moving_averages) > 5000:\n",
    "        moving_averages = moving_averages[-5000:]\n",
    "    subplot_3.clear()\n",
    "    subplot_3.plot(moving_averages)\n",
    "    subplot_3.hlines(0.5, xmin = 0, xmax = len(moving_averages))\n",
    "    subplot_3.set_title('Rolling 200-Tweet Average Sentiment')\n",
    "    subplot_3.set_ylabel('Positivity (0 - 1)')\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, interval=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work\n",
    "\n",
    "There are several ways to extend this work in the future. Some areas we would like to expand upon in the future are:\n",
    "\n",
    "1. Doing predictions for more classes than positive/negative (binary classification)\n",
    "2. Using incoming tweets to adapt the current model in addition to classifying them\n",
    "3. Further exploring text processing with emojis and punctuation (eg. only include punctuation that potentially captures meaning, and filter out the remaining punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "The following research papers were helpful in understanding current work on text processing and sentiment classification techniques:\n",
    "+ Apoor Agarwal -- Sentiment Analysis of Twitter Data.\n",
    "+ Apoor Agarwal, Jasneet Singh Sabharwal -- End-to-End Sentiment Analysis of Twitter Data\n",
    "+ Pang, Lee, and Vaithyanathan -- Thumbs up? Sentiment classification using machine learning techniques\n",
    "\n",
    "The following resources/tutorials were leveraged in constructing our model pipelines and VoteClassifier:\n",
    "+ http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html\n",
    "+ https://pythonprogramming.net/combine-classifier-algorithms-nltk-tutorial/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
